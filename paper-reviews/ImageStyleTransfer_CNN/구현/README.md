# ğŸ§ ë…¼ë¬¸ êµ¬í˜„: Image Style Transfer Using Convolutional Neural Networks

![result](../assets/result2.jpg)  

ë…¼ë¬¸ ë§í¬: https://arxiv.org/abs/1508.06576

ë°œí‘œ í•™íšŒ/ì—°ë„: CVPR 2016 (IEEE Conference on Computer Vision and Pattern Recognition)

ë…¼ë¬¸ ì €ì: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge

------
### Overview

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” CNNì„ ì´ìš©í•˜ì—¬ ì‚¬ì§„ì—ì„œ contentì™€ styleì„ ì¶”ì¶œí•´ë‚´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì„ ì†Œê°œí•˜ì˜€ìŠµë‹ˆë‹¤

ì²˜ìŒìœ¼ë¡œ ë…¼ë¬¸ì„ êµ¬í˜„í•´ë³´ì•˜ìŠµë‹ˆë‹¤ ë…¼ë¬¸ì„ ì½ê³  ì´í•´í•˜ë©° ê·¸ê²ƒì„ ì½”ë“œë¡œ ì˜®ê¸°ëŠ”ë°ê¹Œì§€ ë§ì€ ì‹œê°„ì´ ê±¸ë¦° ê²ƒ ê°™ìŠµë‹ˆë‹¤ ì²«ë²ˆì§¸ë¡œ êµ¬ì¡°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì— ëŒ€í•´ ê³ ë¯¼í–ˆìŠµë‹ˆë‹¤
- `models.py` : ë…¼ë¬¸ì— ë‚˜ì˜¨ ì•„í‚¤í…ì²˜ êµ¬í˜„  
- `loss.py` : style/content loss êµ¬í˜„  
- `train.py` : ì „ì²´ í•™ìŠµ ë£¨í”„

ê´€ìŠµëŒ€ë¡œë¼ë©´ 'dataset.py'ë„ ìˆì–´ì•¼í•˜ì§€ë§Œ ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” datasetì´ë¼ê³  í•˜ê¸°ì—” ì˜¤ë¡œì§€ contentì‚¬ì§„1ì¥, styleì‚¬ì§„ 1ì¥ë§Œì´ í•„ìš”í–ˆê¸°ì— í¬í•¨í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤  
ì¶”ê°€ë¡œ ì´ ë…¼ë¬¸ì„ reviewí•˜ê¸°ë¡œ í•œ ì´ìœ ëŠ” ë…¼ë¬¸ì„ êµ¬í˜„í•œë’¤ ê²°ê³¼ë¬¼ì´ í•œ ëˆˆì— ë³´ì´ë©° cnnì„ ì´ìš©í•˜ì—¬ ì¶”ìƒì ì˜ë¯¸ì¸ styleì„ êµ¬í˜„í•´ëƒˆë‹¤ëŠ” ì ì´ ë§¤ìš° í¥ë¯¸ë¡œì› ê¸°ì— ì„ íƒí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤

-----------

## Models.py

ë…¼ë¬¸ 2. Deep image representationì„ ì‚´í´ë³´ë©´  
>**"The results presented below were generated on the ba
sis of the VGG network"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**"We used the feature
 space provided by a normalised version of the 16 convo
lutional and 5 pooling layers of the 19-layer VGG network"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ì €ìëŠ” <strong>VGG19 model</strong>ì„ ì‚¬ìš©í–ˆë‹¤ê³  í–ˆìŠµë‹ˆë‹¤.
ê·¸ì¤‘ì—ì„œë„ <strong>feature map</strong>ì„ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>**"We reconstruct the input image from from layers â€˜conv1 2â€™ (a),
 â€˜conv2 2â€™ (b), â€˜conv3 2â€™ (c), â€˜conv4 2â€™ (d) and â€˜conv5 2â€™ (e) of the original VGG-Network.k"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**" We reconstruct
 the style of the input image from a style representation built on different subsets of CNN layers ( â€˜conv1 1â€™ (a), â€˜conv1 1â€™ and â€˜conv2 1â€™
 (b), â€˜conv1 1â€™, â€˜conv2 1â€™ and â€˜conv3 1â€™ (c), â€˜conv1 1â€™, â€˜conv2 1â€™, â€˜conv3 1â€™ and â€˜conv4 1â€™ (d), â€˜conv1 1â€™, â€˜conv2 1â€™, â€˜conv3 1â€™, â€˜conv4 1â€™
 and â€˜conv5 1â€™ (e)"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ì–´ë–¤ í”¼ì³ë§µì„ ì‚¬ìš©í–ˆëŠ”ì§€ ê¶ê¸ˆí•´ì§ˆ ë¬´ë µ, ë…¼ë¬¸ ì† ìœ„ ë¬¸ì¥ì—ì„œ <code>conv(a)_(b)</code>ì™€ ê°™ì€ ë§ì´ ë‚˜ì˜¤ëŠ”ë°, VGG19 êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ë©´ ê·¸ ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div style="display: flex; align-items: flex-start;">
  <img src="../assets/paper3.jpg" width="300" style="margin-right: 20px;">
</div>

conv(a)_(b)ëŠ” ì €ìê°€ íŠ¹ì • conv ë ˆì´ì–´ ìœ„ì¹˜ë¥¼ ëª…ì‹œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ  
- `a` : ë¸”ë¡ë²ˆí˜¸(VGGì—ì„œ max poolingìœ¼ë¡œ ë‚˜ëˆ„ë‹ˆ ë¸”ë¡ë“¤)  
- `b` : ê·¸ ë¸”ë¡ ì•ˆì—ì„œì˜ convë ˆì´ì–´ ìˆœì„œ  

ë¼ëŠ”ê±¸ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤

ê·¸ë˜ì„œ ì €ëŠ” styleì‚¬ì§„ì—ì„œ conv1_1, conv2_1, conv3_1, conv4_1, conv5_1ì„, contentì‚¬ì§„ì—ì„œ conv4_2ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤ pytorch vgg19ëª¨ë¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ conv layerë¥¼ ì •ë¦¬í•˜ì˜€ê³  vgg19ë¥¼ í†µê³¼í•˜ë‹¤ê°€ ì›Œì›í•˜ëŠ” convë¥¼ ë§Œë‚˜ê²Œë˜ë©´ í•´ë‹¹ feature mapì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤

```ptyhon
conv = {
    'conv1_1' : 0, #style featuremap layer 
    'conv2_1' : 5, #style featuremap layer 
    'conv3_1' : 10, #style featuremap layer 
    'conv4_1' : 19, #style featuremap layer 
    'conv5_1' : 28, #style featuremap layer 
    'conv4_2' : 21, #content featuremap layer
}
```

### Why CNN?

<div style="display: flex; align-items: flex-start;">
  <img src="../assets/paper4.jpg" width="400" style="margin-right: 20px;">
</div>
-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]
<br><br>

ê·¸ë¦¬ê³  ì´ ì‚¬ì§„ê³¼ í•¨ê»˜ ê¸€ì—ì„œ cnnì„ ì‚¬ìš©í•œ ì´ìœ ë¥¼ ë“¤ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤  <br>

>**We find that reconstruction from lower layers is
 almost perfect (aâ€“c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved(d,e)**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**This creates images that match the style of a given image on an increasing scale while discarding information of the global arrangement of the scene.**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

contentë¥¼ ë½‘ê¸°ìœ„í•´ cnnì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ high-levelì—ì„œ ì„¸ë¶€ì ì¸ í”½ì…€ì •ë³´ê°€ ì†ì‹¤ë˜ì§€ë§Œ ì´ë¯¸ì§€ì˜ ê³ ìˆ˜ì¤€ ë‚´ìš©ì„ ë³´ì¡´í•´ëƒˆê³   
styleì€ cnnì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ì ì  ë” í° ìŠ¤ì¼€ì¼ì—ì„œ ì¼ì¹˜ì‹œí‚¤ëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚´, ì¥ë©´ì˜ ì „ì²´ êµ¬ì„±ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ styleì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤  
ì´ë ‡ê²Œ cnnì´ contentì™€ styleì„ ë½‘ì•„ëƒˆê¸°ì— ì´ëŸ¬í•œ ë…¼ë¬¸ì´ ë‚˜ì˜¬ ìˆ˜ ìˆì—ˆì§€ ì•Šì•˜ë‚˜ ìƒê°ë©ë‹ˆë‹¤
<br><br>

### ì•ì„  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ë„ì½”ë“œ ë§Œë“¤ê¸°

```python
class StyleTransfer:

    ì´ˆê¸°í™”í•  ë•Œ:
        VGG19 ëª¨ë¸ ë¶ˆëŸ¬ì˜´ (pretrained)
        VGG19ì—ì„œ feature ì¶”ì¶œ ë¶€ë¶„ë§Œ ë½‘ìŒ
        ìŠ¤íƒ€ì¼ ì¶”ì¶œì— ì“¸ conv ë ˆì´ì–´ ë²ˆí˜¸ë“¤ ì •í•¨ (ì˜ˆ: conv1_1, conv2_1 ...)
        ì»¨í…ì¸  ì¶”ì¶œì— ì“¸ conv ë ˆì´ì–´ ë²ˆí˜¸ ì •í•¨ (ì˜ˆ: conv4_2)

    forward í•¨ìˆ˜ (ì´ë¯¸ì§€, ëª¨ë“œ):
        featuresë¼ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë§Œë“¦

        ë§Œì•½ ëª¨ë“œê°€ 'style'ì´ë©´:
            VGG19 ë ˆì´ì–´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì§€ë‚˜ê°€ë©´ì„œ
                í˜„ì¬ ë ˆì´ì–´ ë²ˆí˜¸ê°€ ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ëª©ë¡ì— ìˆìœ¼ë©´
                    í•´ë‹¹ ë ˆì´ì–´ ì¶œë ¥ê°’ì„ featuresì— ì¶”ê°€

        ë§Œì•½ ëª¨ë“œê°€ 'content'ì´ë©´:
            VGG19 ë ˆì´ì–´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì§€ë‚˜ê°€ë©´ì„œ
                í˜„ì¬ ë ˆì´ì–´ ë²ˆí˜¸ê°€ ì»¨í…ì¸  ë ˆì´ì–´ ëª©ë¡ì— ìˆìœ¼ë©´
                    í•´ë‹¹ ë ˆì´ì–´ ì¶œë ¥ê°’ì„ featuresì— ì¶”ê°€

        ìµœì¢…ì ìœ¼ë¡œ features ë°˜í™˜
```
<br>

### model ë§Œë“¤ê¸°

```python
#import
import torch
import torch.nn as nn
from torchvision.models import vgg19

conv = {
    'conv1_1' : 0, #style featuremap layer 
    'conv2_1' : 5, #style featuremap layer 
    'conv3_1' : 10, #style featuremap layer 
    'conv4_1' : 19, #style featuremap layer 
    'conv5_1' : 28, #style featuremap layer 
    'conv4_2' : 21, #content featuremap layer
}

class StyleTransfer(nn.Module):
    def __init__(self,):
        super(StyleTransfer, self).__init__()
        #TODO: VGG19 load
        self.vgg19_model = vgg19(pretrained = True)
        self.vgg19_features = self.vgg19_model.features

        #TODO: conv layer ë¶„ë¦¬
        self.style_layer = [conv['conv1_1'], conv['conv2_1'], conv['conv3_1'], conv['conv4_1'], conv['conv5_1']]
        self.content_layer = [conv['conv4_2']]

        pass

    def forward(self, x, mode:str):
        #TODO : style, contentë§ˆë‹¤ conv layer slicingí•´ì„œ ì‚¬ìš©í•˜ê¸°
        features = []

        if mode == 'style':
            for i in range(len(self.vgg19_features)):
                x = self.vgg19_features[i](x)
                if i in self.style_layer:
                    features.append(x)

        if mode == 'content':
            for i in range(len(self.vgg19_features)):
                x = self.vgg19_features[i](x)
                if i in self.content_layer:
                    features.append(x)

        return features
```

----------

## loss.py

ë‹¤ìŒìœ¼ë¡œëŠ” lossë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤<br>

ë…¼ë¬¸ì— ê° content loss, style lossê°€ ë‚˜ì™€ìˆì—ˆê³  ê·¸ ë‘ê°œë¥¼ í•©ì¹œ total_lossê¹Œì§€ ì˜ ì„¤ëª…ë˜ì–´ìˆì—ˆìŠµë‹ˆë‹¤<br><br>

### content loss
```math
\mathcal{L}_{\mathrm{content}}(\vec{p}, \vec{x}, l)
= \frac{1}{2} \sum_{i,j} \left( F^l_{ij} - P^l_{ij} \right)^{2}
```
```math
\frac{\partial \mathcal{L}_{\mathrm{content}}}{\partial F^l_{ij}} =
\begin{cases}
F^l_{ij} - P^l_{ij}, & \text{if } F^l_{ij} > 0, \\
0, & \text{if } F^l_{ij} < 0
\end{cases}
```
<br>
  
- $`\vec{x}`$ : ìƒì„±ëœ ì…ë ¥ ì´ë¯¸ì§€  
- $`\vec{p}`$ : ì›ë³¸ ì´ë¯¸ì§€  
- $`\mathcal{l}`$ : ì´ë¯¸ì§€ ì¸µ  
- $`\mathcal{P}(l)`$, $`\mathcal{F}(l)`$ : ì›ë³¸ ì´ë¯¸ì§€ì™€ ìƒì„±ëœ ì…ë ¥ ì´ë¯¸ì§€ ê° ì´ë¯¸ì§€ $`\mathcal{l}`$ ì¸µì—ì„œì˜ íŠ¹ì§• í‘œí˜„  

ì„ì„ ë…¼ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤ê·¸ëŸ¼ìœ¼ë¡œì¨ MSEë¥¼ êµ¬í•˜ê³  ê·¸ê²ƒìœ¼ë¡œ backpropagationì„ ê³„ì‚°í•´ì„œ ë¬´ì‘ìœ„ ì´ë¯¸ì§€ì¸ $`\vec{x}`$ ë¥¼ ì ì°¨ì ìœ¼ë¡œ ìˆ˜ì •í•´ CNNì˜ íŠ¹ì • ì¸µì—ì„œ ì›ë˜ ì´ë¯¸ì§€ $`\vec{p}`$ ì™€ ë™ì¼í•œ ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ë§Œë“¤ì–´ ì…ë ¥ì´ë¯¸ì§€ì¸ $`\vec{x}`$ ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ì²˜ë¦¬ ê³„ì¸µì„ ë”°ë¼ ì‹¤ì œ ë‚´ìš©ì¸ contentì— ë” ë¯¼ê°í•´ì§€ëŠ” í‘œí˜„ìœ¼ë¡œ ë³€í™˜ë˜ì§€ë§Œ ê·¸ ì •í™•í•œ ì™¸í˜•ì€ ë¶ˆë³€í•´ì§‘ë‹ˆë‹¤ ì¦‰, ë„¤íŠ¸ì›Œí¬ì˜ ë†’ì€ ì¸µë“¤ì€ ì…ë ¥ ì´ë¯¸ì§€ ë‚´ ê°ì²´ì™€ ê·¸ ë°°ì—´ ì¸¡ë©´ì—ì„œ ê³ ìˆ˜ì¤€ì˜ ë‚´ìš©ì„ í¬ì°©í•˜ì§€ë§Œ ë³µì› ì‹œì˜ ì •í™•í•œ í”½ì…€ ê°’ì— ëŒ€í•´ì„œëŠ” í° ì œì•½ì„ ë‘ì§€ ì•Šê³  ì´ì— ë°˜í•´, ë‚®ì€ ì¸µë“¤ë¡œë¶€í„°ì˜ ë³µì›ì€ ì›ë˜ ì´ë¯¸ì§€ì˜ ì •í™•í•œ í”½ì…€ ê°’ì„ ë‹¨ìˆœíˆ ì¬í˜„í•©ë‹ˆë‹¤<br><br><br>

### style loss

```math
G^l_{ij} = \sum_k F^l_{ik} F^l_{jk} \\[1em]
```
```math
E_l = \frac{1}{4 N_l^2 M_l^2} \sum_{i,j} \left( G^l_{ij} - A^l_{ij} \right)^{2} \\[1em]
```
```math
\mathcal{L}_{\mathrm{style}}(\vec{a}, \vec{x}) = \sum_{l=0}^{L} w_l E_l \\[1em]
```
```math
\frac{\partial E_l}{\partial F^l_{ij}} =
\begin{cases}
\frac{1}{N_l^2 M_l^2} \left( (F^l)^{\mathrm{T}} \left( G^l - A^l \right) \right)_{ji}, & \text{if } F^l_{ij} > 0, \\
0, & \text{if } F^l_{ij} < 0
\end{cases}
```
<br>

- $`\mathcal{G}(i,j)`$ : iì™€ j ê°„ì˜ ë‚´ì 
- $`\vec{x}`$ : ì›ë³¸ ì´ë¯¸ì§€  
- $`\mathcal{A}(l)`$, $`\mathcal{F}(l)`$ : ì›ë³¸ ì´ë¯¸ì§€ $`\mathcal{l}`$ ì¸µì—ì„œì˜ íŠ¹ì§• í‘œí˜„  
- $`\mathcal{w}`$ : ê° ì¸µì´ ì „ì²´ ì†ì‹¤ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ê°€ì¤‘ì¹˜ ê³„ìˆ˜  

ì„ì„ ë…¼ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤ ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ì§ˆê° ì •ë³´ë¥¼ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ëœ íŠ¹ì§• ê³µê°„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ íŠ¹ì§• ìƒê´€ê´€ê³„ëŠ” $`\mathcal{G}(l)`$ ì´ ë§Œë“¤ëŸ¬ì…ë‹ˆë‹¤ ìŠ¤íƒ€ì¼ í‘œí˜„ì— í¬í•¨ëœ ê° ì¸µë§ˆë‹¤ $`\mathcal{G}(l)`$ ê³¼ $`\mathcal{A}(l)`$ ê°„ì˜ ìš”ì†Œë³„ í‰ê·  ì œê³±ì˜¤ì°¨ê°€ ê³„ì‚°ë˜ì–´ ìŠ¤íƒ€ì¼ ì†ì‹¤ $`\mathcal{L}_{\text{style}}`$ ì´ ë©ë‹ˆë‹¤<br><br>

### total loss

```math
\mathcal{L}_{\mathrm{total}}(\vec{p}, \vec{a}, \vec{x})
= \alpha\, \mathcal{L}_{\mathrm{content}}(\vec{p}, \vec{x})
+ \beta\, \mathcal{L}_{\mathrm{style}}(\vec{a}, \vec{x})
```
<br>

 - '&alpha;, &beta;' : í•˜ì´í¼ íŒŒë¼ë¯¸í„°

ì´ ì†ì‹¤ $`\mathcal{L}_{\text{total}}`$ ì€ ë‚´ìš© ì†ì‹¤ê³¼ ìŠ¤íƒ€ì¼ ì†ì‹¤ì˜ ì„ í˜• ê²°í•©ë‹ˆì˜€ìŠµë‹ˆë‹¤ ì´ ì´ì†ì‹¤ì„ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ë“¤ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì€ ì˜¤ì°¨ ì—­ì „íŒŒë¡œ ê³„ì‚°ë  ìˆ˜ ìˆì—ˆìœ¼ë©° ì´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì´ë¯¸ $`\vec{x}`$ ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°±ì‹ í•˜ëŠ”ë° ì‚¬ìš©ë˜ê³  ê²°êµ­ ìŠ¤íƒ€ì¼ $`\vec{a}`$ ì˜ ìŠ¤íƒ€ì¼ íŠ¹ì§•ê³¼ ë‚´ìš© ì´ë¯¸ì§€ $`\vec{p}`$ ì˜ ë‚´ìš© íŠ¹ì§•ì„ ë™ì‹œì— ì¼ì¹˜ì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤

ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ resultë¶€ë¶„ì—ì„œ

>**The ratio Î±/Î² was either 1 Ã— 10^âˆ’3 (Fig 3 B), 8 Ã— 10^âˆ’4 (Fig 3
 C), 5 Ã—10^âˆ’3 (Fig 3 D), or 5Ã—10^âˆ’4 (Fig 3 E,F).  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ê° ì‚¬ì§„ë§ˆë‹¤ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°’ì„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì˜€ë‹¤ëŠ” ê²ƒ ë˜í•œ ì–»ì–´ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤
<br><br>

### loss ë§Œë“¤ê¸°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

#contentlossì •ì˜
class ContentLoss(nn.Module):
    def __init__(self,):
        super(ContentLoss, self).__init__()

    def forward(self, x, y):
        loss = F.mse_loss(x,y)
        return loss

#stylelossì •ì˜
class StyleLoss(nn.Module):
    def __init__(self,):
        super(StyleLoss, self).__init__()

    def gram_matrix(self, x):
        b,c,h,w = x.size()
        featrues = x.view(b,c,h*w)
        features_T = featrues.transpose(1,2)
        G = torch.matmul(featrues, features_T)

        return G.div(b*c*h*w)

    def forward(self, x, y):

        Gx = self.gram_matrix(x)
        Gy = self.gram_matrix(y)
        loss = F.mse_loss(x, y)
        return loss
```
----------

## train.py

ë§ˆì§€ë§‰ìœ¼ë¡œ train.pyë¥¼ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤
<br>

1. preprocessing
    - ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 512Ã—512ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³ , ToTensor()ë¡œ float32 í…ì„œë¡œ ë°”ê¿‰ë‹ˆë‹¤. ê²°ê³¼ í˜•íƒœëŠ” (1, C, H, W)ì…ë‹ˆë‹¤.
    - ImageNet í†µê³„(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])ë¡œ ì •ê·œí™”í•˜ì—¬ VGG ê¸°ë°˜ íŠ¹ì„± ì¶”ì¶œì— ë§ì¶¥ë‹ˆë‹¤.
    - ìµœì¢… ë°˜í™˜ì€ ë°°ì¹˜ ì°¨ì›ì„ ë¶™ì¸ í…ì„œë¡œ, ë°”ë¡œ ëª¨ë¸ì— ë„£ì„ ìˆ˜ ìˆëŠ” í˜•íƒœì…ë‹ˆë‹¤.

2. postprocessing
    - í•™ìŠµ ì¤‘ì¸ ìƒì„± ì´ë¯¸ì§€ í…ì„œë¥¼ CPUë¡œ ì˜®ê¸°ê³  numpyë¡œ ë³€í™˜í•œ ë’¤, (H,W,C)ë¡œ ì „ì¹˜í•©ë‹ˆë‹¤.
    - ì •ê·œí™”ì˜ ì—­ë³€í™˜(denorm)ì„ ì ìš©í•´ ì›ë˜ í”½ì…€ ìŠ¤ì¼€ì¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤: image = image*std + mean í›„ [0,1]ë¡œ í´ë¦¬í•‘í•˜ê³  uint8ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - PIL.Imageë¡œ ë‹¤ì‹œ ë°”ê¿”ì„œ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

3. ëª¨ë¸/ì†ì‹¤ êµ¬ì„±
    - StyleTransfer()ëŠ” ë°±ë³¸ì—ì„œ ì¤‘ê°„ featureë¥¼ ë½‘ì•„, ëª¨ë“œì— ë”°ë¼ ë‘ ê°€ì§€ í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.
        - `content`: ì½˜í…ì¸  ì†ì‹¤ ê³„ì‚°ìš© feature ë¦¬ìŠ¤íŠ¸
        - `style`: ìŠ¤íƒ€ì¼ ì†ì‹¤ ê³„ì‚°ìš© feature ë¦¬ìŠ¤íŠ¸(ë³´í†µ Gram matrix ê³„ì‚°ì— ì í•©í•œ ë ˆì´ì–´ë“¤)
    - ContentLoss, StyleLossëŠ” ê°ê° ì½˜í…ì¸  ì¼ì¹˜, ìŠ¤íƒ€ì¼ ì¼ì¹˜ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ì½”ë“œ ë³¸ë¬¸ì—ì„œëŠ” ê° ë ˆì´ì–´ë³„ ì†ì‹¤ì„ í•©ì‚°í•©ë‹ˆë‹¤.

4. í•˜ì´í¼íŒŒë¼ë¯¸í„°
    - `alpha=1` : ì½˜í…ì¸  ì†ì‹¤ ê°€ì¤‘ì¹˜
    - `beta=1e6`: ìŠ¤íƒ€ì¼ ì†ì‹¤ ê°€ì¤‘ì¹˜
    - `lr=1` : í”½ì…€ì„ ì§ì ‘ ìµœì í™”í•˜ë¯€ë¡œ ë¹„êµì  í° í•™ìŠµë¥ ì„ ì‚¬ìš©

5. ë””ë°”ì´ìŠ¤ ì„¤ì •
    - GPUê°€ ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ cpuì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ê³¼ ì…ë ¥ í…ì„œë¥¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê²¨ ì¼ê´€ì„± ìˆê²Œ ì—°ì‚°í•©ë‹ˆë‹¤.

6. ì´ˆê¸° ì´ë¯¸ì§€ ì„¤ì •
    - ë…¼ë¬¸ìƒì—ì„œëŠ” white imageë¥¼ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ ì¡°ê¸ˆ ë” ì¢‹ì€ ê²°ê³¼ë¬¼ì„ ì–»ê¸°ìœ„í•´ ì´ë¯¸ì§€ ê·¸ìì²´(content image)ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
    - x.requires_grad_(True)ë¡œ í”½ì…€ì„ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ ì˜µí‹°ë§ˆì´ì €ê°€ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

7. ì˜µí‹°ë§ˆì´ì €
    - optim.Adam([x], lr=lr)ë¡œ ë‹¨ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° ì§‘í•©(ì´ë¯¸ì§€ í…ì„œ x)ë§Œ ìµœì í™”í•©ë‹ˆë‹¤.

8. ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½
   1. ì½˜í…ì¸ /ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©
   2. ë¯¸ë¦¬ í•™ìŠµëœ íŠ¹ì„± ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬ë¡œ ì½˜í…ì¸ /ìŠ¤íƒ€ì¼ íƒ€ê¹ƒ í‘œí˜„ ê³ ì •
   3. ì´ˆê¸° ì´ë¯¸ì§€ xë¥¼ ë³€ìˆ˜ë¡œ ë‘ê³ , ì½˜í…ì¸ /ìŠ¤íƒ€ì¼ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ xë¥¼ ì§ì ‘ ìµœì í™”
   4. ì£¼ê¸°ì ìœ¼ë¡œ ì¤‘ê°„ ê²°ê³¼ë¥¼ ë³µì›í•´ ì €ì¥í•˜ë©° ìˆ˜ë ´ ìƒí™©ì„ í™•ì¸
<br>

```python
#import
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
import os

import numpy as np
from PIL import Image

from models import StyleTransfer
from loss import ContentLoss, StyleLoss
from tqdm import tqdm


mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

def pre_processing(image:Image.Image) -> torch.Tensor:
    preprocessing = T.Compose([
        T.Resize((512,512)), #ì´ë¯¸ì§€ resize
        T.ToTensor(), #image to tensor
        T.Normalize(mean, std) # lambda x : (x-mean) / std
    ]) # (c, h ,w)

    # (1, c, h ,w)
    image_tensor:torch.Tensor = preprocessing(image)

    return image_tensor.unsqueeze(0)

def post_processing(tensor:torch.Tensor) -> Image.Image:

    # shape 1,c,h,w
    image:np.ndarray = tensor.to('cpu').detach().numpy()
    # shape c,h,w
    image = image.squeeze()
    # shape h,w,c
    image = image.transpose(1, 2, 0)
    # de norm
    image = image*std + mean
    # clip
    image = image.clip(0,1)*255
    # dtype uint8
    image = image.astype(np.uint8)
    # numpy -> Image
    return Image.fromarray(image)


def train_main():
    # load data
    content_image = Image.open('./content.jpg')
    content_image = pre_processing(content_image)

    style_image = Image.open('./style.jpg')
    style_image = pre_processing(style_image)

    # load model
    style_transfer = StyleTransfer().eval()

    # load loss
    content_loss = ContentLoss()
    style_loss = StyleLoss()

    # hyper parameter
    alpha = 1
    beta = 1e6
    lr = 1

    save_root = f'{alpha}_{beta}_{lr}_style_transfer_01'
    os.makedirs(save_root, exist_ok=True)

    # device setting
    device = 'cpu'
    if torch.cuda.is_available():
        device = 'cuda'


    style_transfer = style_transfer.to(device)
    content_image = content_image.to(device)
    style_image = style_image.to(device)
    
    # noise
    # x = torch.randn(1,3,512,512).to(device) ë…¸ì´ì¦ˆ ì‹œì‘ì‚¬ì§„
    x = content_image.clone()                #ê³ ì–‘ì´ ì‹œì‘ì‚¬ì§„
    x.requires_grad_(True)

    # setting optimizer
    optimizer = optim.Adam([x], lr=lr)

    # train loop
    steps = 500
    for step in tqdm(range(steps)):
        ## content representation (x, content_image)
        ## style representation (x, style_image)

        x_content_list = style_transfer(x, 'content')
        y_content_list = style_transfer(content_image, 'content')

        x_style_list = style_transfer(x, 'style')
        y_style_list = style_transfer(style_image, 'style')

        ## loss_content, loss_style
        loss_c = 0
        loss_s = 0
        loss_total = 0

        for x_content, y_content in zip(x_content_list, y_content_list):
            loss_c += content_loss(x_content, y_content)
        loss_c = alpha*loss_c

        for x_style, y_style in zip(x_style_list, y_style_list):
            loss_s += style_loss(x_style, y_style)
        loss_s = beta*loss_s

        loss_total = loss_c + loss_s

        ## optimizer step
        optimizer.zero_grad()
        loss_total.backward()
        optimizer.step()

        ## loss print
        if step%100==0:
            print(f"loss_c: {loss_c.cpu()}")
            print(f"loss_s: {loss_s.cpu()}")
            print(f"loss_total: {loss_total.cpu()}")

            ## post processing
            ## image gen output save
            gen_img:Image.Image = post_processing(x)
            gen_img.save(os.path.join(save_root, f'{step}.jpg'))

if __name__=="__main__":
    train_main()
```

--------

## ìˆ˜í–‰ê²°ê³¼

ìˆ˜í–‰ì€ codeëŠ” colabì—ì„œ ì‹¤í–‰ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤
<br>

![result](../assets/result3.jpg)  

ë‹¤ìŒì‚¬ì§„ì€ ì œê°€ êµ¬í˜„í•œ ì‹¤ì œ ì½”ë“œë¡œ ë§Œë“  ì‚¬ì§„ì…ë‹ˆë‹¤ content ì´ë¯¸ì§€ëŠ” ê·€ì—¬ìš´ ê³ ì–‘ì´ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©° styleì´ë¯¸ì§€ë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ê³¼ ìƒ‰ìƒì´ ê°€ë¯¸ë˜ì–´ìˆëŠ” íŒ¨í„´ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤ ë‘ ì‚¬ì§„ì˜ ê²°ê³¼ë¬¼ ë‹¤ 100epochì§¸ ì‚¬ì§„ì´ë©° ê²°ê³¼ê°€ ë§¤ìš° ì˜ ë‚˜ì˜¨ ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤
<br>

<div style="display: flex; align-items: flex-start;">
  <img src="../assets/result4.jpg" width="400" style="margin-right: 20px;">
</div>
<br>
ì‹¤ì œ lossê°’ì„ í™•ì¸í•´ë³´ë©´ 100epochë§Œì— total lossê°€ ìˆ˜ë ´í•œê±¸ í™•ì¸ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤

---------
### ì†Œê°

ë…¼ë¬¸ì„ ì½ê³  ì´ë ‡ê²Œ ì½”ë”©ê¹Œì§€í•˜ëŠ” ê³¼ì •ì´ ì²˜ìŒí•´ë³´ë©´ì„œ ì˜ˆìƒë³´ë‹¤ í›¨ì”¬ ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ëŠê¼ˆìŠµë‹ˆë‹¤ ë…¼ë¬¸ ì† ìˆ˜ì‹ê³¼ ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì½ê³  ì°¸ê³  ìë£Œë¥¼ ì°¾ì•„ë³´ì•˜ìœ¼ë©°, ì´ë¥¼ ì½”ë“œë¡œ ì˜®ê¸°ëŠ” ê³¼ì •ì—ì„œ ì‹œí–‰ì°©ì˜¤ë„ ë§ì•˜ìŠµë‹ˆë‹¤ íŠ¹íˆ ë…¼ë¬¸ê³¼ ì‹¤ì œ êµ¬í˜„ê°„ì˜ ì„¸ë¶€ ì°¨ì´ë¥¼ ë§ì¶”ëŠ” ì¼ì´ ì œì¼ í˜ë“¤ì—ˆê³  ê·¸ ê³¼ì •ì—ì„œ ëª¨ë¸ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ë¥¼ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤ í˜ë“¤ì—ˆì§€ë§Œ ì´ ê²½í—˜ì„ í†µí•´ ë‹¨ìˆœí•œ ì½ê¸°ì— ê·¸ì¹˜ì§€ ì•Šê³ , ì—°êµ¬ ë‚´ìš©ì„ ì‘ìš©í•´ë³´ê³ , ë” ë§ì€ ë…¼ë¬¸ì„ ì½ì–´ ì•ìœ¼ë¡œ ê³µë¶€ì™€ ì—°êµ¬ì— í° ìì‚°ì´ ë  ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤
