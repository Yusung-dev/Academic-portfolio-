# ğŸ§ ë…¼ë¬¸ êµ¬í˜„: Image Style Transfer Using Convolutional Neural Networks

![result](../assets/result2.jpg)  

ë…¼ë¬¸ ë§í¬: https://arxiv.org/abs/1508.06576

ë°œí‘œ í•™íšŒ/ì—°ë„: CVPR 2016 (IEEE Conference on Computer Vision and Pattern Recognition)

ë…¼ë¬¸ ì €ì: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge

------
### Overview

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” CNNì„ ì´ìš©í•˜ì—¬ ì‚¬ì§„ì—ì„œ contentì™€ styleì„ ì¶”ì¶œí•´ë‚´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì„ ì†Œê°œí•˜ì˜€ìŠµë‹ˆë‹¤

ì²˜ìŒìœ¼ë¡œ ë…¼ë¬¸ì„ êµ¬í˜„í•´ë³´ì•˜ìŠµë‹ˆë‹¤ ë…¼ë¬¸ì„ ì½ê³  ì´í•´í•˜ë©° ê·¸ê²ƒì„ ì½”ë“œë¡œ ì˜®ê¸°ëŠ”ë°ê¹Œì§€ ë§ì€ ì‹œê°„ì´ ê±¸ë¦° ê²ƒ ê°™ìŠµë‹ˆë‹¤ ì²«ë²ˆì§¸ë¡œ êµ¬ì¡°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì— ëŒ€í•´ ê³ ë¯¼í–ˆìŠµë‹ˆë‹¤
- `models.py` : ë…¼ë¬¸ì— ë‚˜ì˜¨ ì•„í‚¤í…ì²˜ êµ¬í˜„  
- `loss.py` : style/content loss êµ¬í˜„  
- `train.py` : ì „ì²´ í•™ìŠµ ë£¨í”„

ê´€ìŠµëŒ€ë¡œë¼ë©´ 'dataset.py'ë„ ìˆì–´ì•¼í•˜ì§€ë§Œ ì´ë²ˆ ë…¼ë¬¸ì—ì„œëŠ” datasetì´ë¼ê³  í•˜ê¸°ì—” ì˜¤ë¡œì§€ contentì‚¬ì§„1ì¥, styleì‚¬ì§„ 1ì¥ë§Œì´ í•„ìš”í–ˆê¸°ì— í¬í•¨í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤  
ì¶”ê°€ë¡œ ì´ ë…¼ë¬¸ì„ reviewí•˜ê¸°ë¡œ í•œ ì´ìœ ëŠ” ë…¼ë¬¸ì„ êµ¬í˜„í•œë’¤ ê²°ê³¼ë¬¼ì´ í•œ ëˆˆì— ë³´ì´ë©° cnnì„ ì´ìš©í•˜ì—¬ ì¶”ìƒì ì˜ë¯¸ì¸ styleì„ êµ¬í˜„í•´ëƒˆë‹¤ëŠ” ì ì´ ë§¤ìš° í¥ë¯¸ë¡œì› ê¸°ì— ì„ íƒí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤

-----------

## Models.py

ë…¼ë¬¸ 2. Deep image representationì„ ì‚´í´ë³´ë©´  
>**"The results presented below were generated on the ba
sis of the VGG network"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**"We used the feature
 space provided by a normalised version of the 16 convo
lutional and 5 pooling layers of the 19-layer VGG network"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ì €ìëŠ” <strong>VGG19 model</strong>ì„ ì‚¬ìš©í–ˆë‹¤ê³  í–ˆìŠµë‹ˆë‹¤.
ê·¸ì¤‘ì—ì„œë„ <strong>feature map</strong>ì„ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

>**"We reconstruct the input image from from layers â€˜conv1 2â€™ (a),
 â€˜conv2 2â€™ (b), â€˜conv3 2â€™ (c), â€˜conv4 2â€™ (d) and â€˜conv5 2â€™ (e) of the original VGG-Network.k"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**" We reconstruct
 the style of the input image from a style representation built on different subsets of CNN layers ( â€˜conv1 1â€™ (a), â€˜conv1 1â€™ and â€˜conv2 1â€™
 (b), â€˜conv1 1â€™, â€˜conv2 1â€™ and â€˜conv3 1â€™ (c), â€˜conv1 1â€™, â€˜conv2 1â€™, â€˜conv3 1â€™ and â€˜conv4 1â€™ (d), â€˜conv1 1â€™, â€˜conv2 1â€™, â€˜conv3 1â€™, â€˜conv4 1â€™
 and â€˜conv5 1â€™ (e)"**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ì–´ë–¤ í”¼ì³ë§µì„ ì‚¬ìš©í–ˆëŠ”ì§€ ê¶ê¸ˆí•´ì§ˆ ë¬´ë µ, ë…¼ë¬¸ ì† ìœ„ ë¬¸ì¥ì—ì„œ <code>conv(a)_(b)</code>ì™€ ê°™ì€ ë§ì´ ë‚˜ì˜¤ëŠ”ë°, VGG19 êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ë©´ ê·¸ ì˜ë¯¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div style="display: flex; align-items: flex-start;">
  <img src="../assets/paper3.jpg" width="300" style="margin-right: 20px;">
</div>

conv(a)_(b)ëŠ” ì €ìê°€ íŠ¹ì • conv ë ˆì´ì–´ ìœ„ì¹˜ë¥¼ ëª…ì‹œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ  
- `a` : ë¸”ë¡ë²ˆí˜¸(VGGì—ì„œ max poolingìœ¼ë¡œ ë‚˜ëˆ„ë‹ˆ ë¸”ë¡ë“¤)  
- `b` : ê·¸ ë¸”ë¡ ì•ˆì—ì„œì˜ convë ˆì´ì–´ ìˆœì„œ  

ë¼ëŠ”ê±¸ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤

ê·¸ë˜ì„œ ì €ëŠ” styleì‚¬ì§„ì—ì„œ conv1_1, conv2_1, conv3_1, conv4_1, conv5_1ì„, contentì‚¬ì§„ì—ì„œ conv4_2ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤ pytorch vgg19ëª¨ë¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ conv layerë¥¼ ì •ë¦¬í•˜ì˜€ê³  vgg19ë¥¼ í†µê³¼í•˜ë‹¤ê°€ ì›Œì›í•˜ëŠ” convë¥¼ ë§Œë‚˜ê²Œë˜ë©´ í•´ë‹¹ feature mapì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤

```ptyhon
conv = {
    'conv1_1' : 0, #style featuremap layer 
    'conv2_1' : 5, #style featuremap layer 
    'conv3_1' : 10, #style featuremap layer 
    'conv4_1' : 19, #style featuremap layer 
    'conv5_1' : 28, #style featuremap layer 
    'conv4_2' : 21, #content featuremap layer
}
```

### Why CNN?

<div style="display: flex; align-items: flex-start;">
  <img src="../assets/paper4.jpg" width="400" style="margin-right: 20px;">
</div>
-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]
<br><br>

ê·¸ë¦¬ê³  ì´ ì‚¬ì§„ê³¼ í•¨ê»˜ ê¸€ì—ì„œ cnnì„ ì‚¬ìš©í•œ ì´ìœ ë¥¼ ë“¤ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤  <br>

>**We find that reconstruction from lower layers is
 almost perfect (aâ€“c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved(d,e)**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

>**This creates images that match the style of a given image on an increasing scale while discarding information of the global arrangement of the scene.**  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

contentë¥¼ ë½‘ê¸°ìœ„í•´ cnnì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ high-levelì—ì„œ ì„¸ë¶€ì ì¸ í”½ì…€ì •ë³´ê°€ ì†ì‹¤ë˜ì§€ë§Œ ì´ë¯¸ì§€ì˜ ê³ ìˆ˜ì¤€ ë‚´ìš©ì„ ë³´ì¡´í•´ëƒˆê³   
styleì€ cnnì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì£¼ì–´ì§„ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì„ ì ì  ë” í° ìŠ¤ì¼€ì¼ì—ì„œ ì¼ì¹˜ì‹œí‚¤ëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚´, ì¥ë©´ì˜ ì „ì²´ êµ¬ì„±ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ styleì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤  
ì´ë ‡ê²Œ cnnì´ contentì™€ styleì„ ë½‘ì•„ëƒˆê¸°ì— ì´ëŸ¬í•œ ë…¼ë¬¸ì´ ë‚˜ì˜¬ ìˆ˜ ìˆì—ˆì§€ ì•Šì•˜ë‚˜ ìƒê°ë©ë‹ˆë‹¤
<br><br>

### ì•ì„  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ë„ì½”ë“œ ë§Œë“¤ê¸°

```python
class StyleTransfer:

    ì´ˆê¸°í™”í•  ë•Œ:
        VGG19 ëª¨ë¸ ë¶ˆëŸ¬ì˜´ (pretrained)
        VGG19ì—ì„œ feature ì¶”ì¶œ ë¶€ë¶„ë§Œ ë½‘ìŒ
        ìŠ¤íƒ€ì¼ ì¶”ì¶œì— ì“¸ conv ë ˆì´ì–´ ë²ˆí˜¸ë“¤ ì •í•¨ (ì˜ˆ: conv1_1, conv2_1 ...)
        ì»¨í…ì¸  ì¶”ì¶œì— ì“¸ conv ë ˆì´ì–´ ë²ˆí˜¸ ì •í•¨ (ì˜ˆ: conv4_2)

    forward í•¨ìˆ˜ (ì´ë¯¸ì§€, ëª¨ë“œ):
        featuresë¼ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë§Œë“¦

        ë§Œì•½ ëª¨ë“œê°€ 'style'ì´ë©´:
            VGG19 ë ˆì´ì–´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì§€ë‚˜ê°€ë©´ì„œ
                í˜„ì¬ ë ˆì´ì–´ ë²ˆí˜¸ê°€ ìŠ¤íƒ€ì¼ ë ˆì´ì–´ ëª©ë¡ì— ìˆìœ¼ë©´
                    í•´ë‹¹ ë ˆì´ì–´ ì¶œë ¥ê°’ì„ featuresì— ì¶”ê°€

        ë§Œì•½ ëª¨ë“œê°€ 'content'ì´ë©´:
            VGG19 ë ˆì´ì–´ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ì§€ë‚˜ê°€ë©´ì„œ
                í˜„ì¬ ë ˆì´ì–´ ë²ˆí˜¸ê°€ ì»¨í…ì¸  ë ˆì´ì–´ ëª©ë¡ì— ìˆìœ¼ë©´
                    í•´ë‹¹ ë ˆì´ì–´ ì¶œë ¥ê°’ì„ featuresì— ì¶”ê°€

        ìµœì¢…ì ìœ¼ë¡œ features ë°˜í™˜
```
<br>

### model ë§Œë“¤ê¸°

```python
#import
import torch
import torch.nn as nn
from torchvision.models import vgg19

conv = {
    'conv1_1' : 0, #style featuremap layer 
    'conv2_1' : 5, #style featuremap layer 
    'conv3_1' : 10, #style featuremap layer 
    'conv4_1' : 19, #style featuremap layer 
    'conv5_1' : 28, #style featuremap layer 
    'conv4_2' : 21, #content featuremap layer
}

class StyleTransfer(nn.Module):
    def __init__(self,):
        super(StyleTransfer, self).__init__()
        #TODO: VGG19 load
        self.vgg19_model = vgg19(pretrained = True)
        self.vgg19_features = self.vgg19_model.features

        #TODO: conv layer ë¶„ë¦¬
        self.style_layer = [conv['conv1_1'], conv['conv2_1'], conv['conv3_1'], conv['conv4_1'], conv['conv5_1']]
        self.content_layer = [conv['conv4_2']]

        pass

    def forward(self, x, mode:str):
        #TODO : style, contentë§ˆë‹¤ conv layer slicingí•´ì„œ ì‚¬ìš©í•˜ê¸°
        features = []

        if mode == 'style':
            for i in range(len(self.vgg19_features)):
                x = self.vgg19_features[i](x)
                if i in self.style_layer:
                    features.append(x)

        if mode == 'content':
            for i in range(len(self.vgg19_features)):
                x = self.vgg19_features[i](x)
                if i in self.content_layer:
                    features.append(x)

        return features
```

----------

## loss.py

ë‹¤ìŒìœ¼ë¡œëŠ” lossë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤<br>

ë…¼ë¬¸ì— ê° content loss, style lossê°€ ë‚˜ì™€ìˆì—ˆê³  ê·¸ ë‘ê°œë¥¼ í•©ì¹œ total_lossê¹Œì§€ ì˜ ì„¤ëª…ë˜ì–´ìˆì—ˆìŠµë‹ˆë‹¤<br><br>

### content loss
<div style="display: flex; align-items: flex-start;">
  <img src="../assets/cal1.jpg" width="300" style="margin-right: 20px;">
  <br>
  <img src="../assets/cal2.jpg" width="300" style="margin-right: 20px;">
</div>
  
- `x` : ìƒì„±ëœ ì…ë ¥ ì´ë¯¸ì§€  
- `p` : ì›ë³¸ ì´ë¯¸ì§€  
- `l` : ì´ë¯¸ì§€ ì¸µ  
- `P(l), F(l)` : ì›ë³¸ ì´ë¯¸ì§€ì™€ ìƒì„±ëœ ì…ë ¥ ì´ë¯¸ì§€ ê° ì´ë¯¸ì§€ lì¸µì—ì„œì˜ íŠ¹ì§• í‘œí˜„  

ì„ì„ ë…¼ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤ê·¸ëŸ¼ìœ¼ë¡œì¨ mseë¥¼ êµ¬í•˜ê³  ê·¸ê²ƒìœ¼ë¡œ backpropagationì„ ê³„ì‚°í•´ì„œ ë¬´ì‘ìœ„ ì´ë¯¸ì§€ì¸ xë¥¼ ì ì°¨ì ìœ¼ë¡œ ìˆ˜ì •í•´ cnnì˜ íŠ¹ì • ì¸µì—ì„œ ì›ë˜ ì´ë¯¸ì§€ pì™€ ë™ì¼í•œ ë°˜ì‘ì„ ìƒì„±í•˜ë„ë¡ ë§Œë“¤ì–´ ì…ë ¥ì´ë¯¸ì§€ì¸ xëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ì²˜ë¦¬ ê³„ì¸µì„ ë”°ë¼ ì‹¤ì œ ë‚´ìš©ì¸ contentì— ë” ë¯¼ê°í•´ì§€ëŠ” í‘œí˜„ìœ¼ë¡œ ë³€í™˜ë˜ì§€ë§Œ ê·¸ ì •í™•í•œ ì™¸í˜•ì€ ë¶ˆë³€í•´ì§‘ë‹ˆë‹¤ ì¦‰, ë„¤íŠ¸ì›Œí¬ì˜ ë†’ì€ ì¸µë“¤ì€ ì…ë ¥ ì´ë¯¸ì§€ ë‚´ ê°ì²´ì™€ ê·¸ ë°°ì—´ ì¸¡ë©´ì—ì„œ ê³ ìˆ˜ì¤€ì˜ ë‚´ìš©ì„ í¬ì°©í•˜ì§€ë§Œ ë³µì› ì‹œì˜ ì •í™•í•œ í”½ì…€ ê°’ì— ëŒ€í•´ì„œëŠ” í° ì œì•½ì„ ë‘ì§€ ì•Šê³  ì´ì— ë°˜í•´, ë‚®ì€ ì¸µë“¤ë¡œë¶€í„°ì˜ ë³µì›ì€ ì›ë˜ ì´ë¯¸ì§€ì˜ ì •í™•í•œ í”½ì…€ ê°’ì„ ë‹¨ìˆœíˆ ì¬í˜„í•©ë‹ˆë‹¤<br><br><br>

### style loss
<div style="display: flex; align-items: flex-start;">
  <img src="../assets/cal3.jpg" width="150" style="margin-right: 20px;">
  <br>
  <img src="../assets/cal4.jpg" width="200" style="margin-right: 20px;">
  <br>
  <img src="../assets/cal5.jpg" width="200" style="margin-right: 20px;">
  <br>
  <img src="../assets/cal6.jpg" width="300" style="margin-right: 20px;">
</div>
<br>
  
- `G(i,j)` : iì™€ j ê°„ì˜ ë‚´ì 
- `a` : ì›ë³¸ ì´ë¯¸ì§€  
- `A(l), F(l)` : ì›ë³¸ ì´ë¯¸ì§€ lì¸µì—ì„œì˜ íŠ¹ì§• í‘œí˜„  
- `w` : ê° ì¸µì´ ì „ì²´ ì†ì‹¤ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ê°€ì¤‘ì¹˜ ê³„ìˆ˜  

ì„ì„ ë…¼ë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤ ì…ë ¥ ì´ë¯¸ì§€ì˜ ìŠ¤íƒ€ì¼ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ì§ˆê° ì •ë³´ë¥¼ í¬ì°©í•˜ë„ë¡ ì„¤ê³„ëœ íŠ¹ì§• ê³µê°„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ íŠ¹ì§• ìƒê´€ê´€ê³„ëŠ” G(l)ì´ ë§Œë“¤ëŸ¬ì…ë‹ˆë‹¤ ìŠ¤íƒ€ì¼ í‘œí˜„ì— í¬í•¨ëœ ê° ì¸µë§ˆë‹¤ G(l)ê³¼ A(l)ê°„ì˜ ìš”ì†Œë³„ í‰ê·  ì œê³±ì˜¤ì°¨ê°€ ê³„ì‚°ë˜ì–´ ìŠ¤íƒ€ì¼ ì†ì‹¤ L(style)ì´ ë©ë‹ˆë‹¤<br><br><br>

### total loss
<div style="display: flex; align-items: flex-start;">
  <img src="../assets/cal7.jpg" width="300" style="margin-right: 20px;">
 <br>

 - '&alpha;, &beta;' : í•˜ì´í¼ íŒŒë¼ë¯¸í„°

ì´ ì†ì‹¤ L(total)ì€ ë‚´ìš© ì†ì‹¤ê³¼ ìŠ¤íƒ€ì¼ ì†ì‹¤ì˜ ì„ í˜• ê²°í•©ë‹ˆì˜€ìŠµë‹ˆë‹¤ ì´ ì´ì†ì‹¤ì„ ì´ë¯¸ì§€ì˜ í”½ì…€ ê°’ë“¤ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì€ ì˜¤ì°¨ ì—­ì „íŒŒë¡œ ê³„ì‚°ë  ìˆ˜ ìˆì—ˆìœ¼ë©° ì´ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì´ë¯¸ xë¥¼ ë°˜ë³µì ìœ¼ë¡œ ê°±ì‹ í•˜ëŠ”ë° ì‚¬ìš©ë˜ê³  ê²°êµ­ ìŠ¤íƒ€ì¼ aì˜ ìŠ¤íƒ€ì¼ íŠ¹ì§•ê³¼ ë‚´ìš© ì´ë¯¸ì§€ pì˜ ë‚´ìš© íŠ¹ì§•ì„ ë™ì‹œì— ì¼ì¹˜ì‹œí‚¬ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤

ì¶”ê°€ì ìœ¼ë¡œ ë…¼ë¬¸ resultë¶€ë¶„ì—ì„œ

>**The ratio Î±/Î² was either 1 Ã— 10^âˆ’3 (Fig 3 B), 8 Ã— 10^âˆ’4 (Fig 3
 C), 5 Ã—10^âˆ’3 (Fig 3 D), or 5Ã—10^âˆ’4 (Fig 3 E,F).  
>-[Gatys et al., Image Style Transfer Using CNNs, CVPR 2016]

ê° ì‚¬ì§„ë§ˆë‹¤ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°’ì„ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì˜€ë‹¤ëŠ” ê²ƒ ë˜í•œ ì–»ì–´ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤
<br><br>

### loss ë§Œë“¤ê¸°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

#contentlossì •ì˜
class ContentLoss(nn.Module):
    def __init__(self,):
        super(ContentLoss, self).__init__()

    def forward(self, x, y):
        loss = F.mse_loss(x,y)
        return loss

#stylelossì •ì˜
class StyleLoss(nn.Module):
    def __init__(self,):
        super(StyleLoss, self).__init__()

    def gram_matrix(self, x):
        b,c,h,w = x.size()
        featrues = x.view(b,c,h*w)
        features_T = featrues.transpose(1,2)
        G = torch.matmul(featrues, features_T)

        return G.div(b*c*h*w)

    def forward(self, x, y):

        Gx = self.gram_matrix(x)
        Gy = self.gram_matrix(y)
        loss = F.mse_loss(x, y)
        return loss
```
----------

## train.py

```python
#import
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
import os

import numpy as np
from PIL import Image

from models import StyleTransfer
from loss import ContentLoss, StyleLoss
from tqdm import tqdm


mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

def pre_processing(image:Image.Image) -> torch.Tensor:
    preprocessing = T.Compose([
        T.Resize((512,512)), #ì´ë¯¸ì§€ resize
        T.ToTensor(), #image to tensor
        T.Normalize(mean, std) # lambda x : (x-mean) / std
    ]) # (c, h ,w)

    # (1, c, h ,w)
    image_tensor:torch.Tensor = preprocessing(image)

    return image_tensor.unsqueeze(0)

def post_processing(tensor:torch.Tensor) -> Image.Image:

    # shape 1,c,h,w
    image:np.ndarray = tensor.to('cpu').detach().numpy()
    # shape c,h,w
    image = image.squeeze()
    # shape h,w,c
    image = image.transpose(1, 2, 0)
    # de norm
    image = image*std + mean
    # clip
    image = image.clip(0,1)*255
    # dtype uint8
    image = image.astype(np.uint8)
    # numpy -> Image
    return Image.fromarray(image)


def train_main():
    # load data
    content_image = Image.open('./content.jpg')
    content_image = pre_processing(content_image)

    style_image = Image.open('./style.jpg')
    style_image = pre_processing(style_image)

    # load model
    style_transfer = StyleTransfer().eval()

    # load loss
    content_loss = ContentLoss()
    style_loss = StyleLoss()

    # hyper parameter
    alpha = 1
    beta = 1e6
    lr = 1

    save_root = f'{alpha}_{beta}_{lr}_style_transfer_01'
    os.makedirs(save_root, exist_ok=True)

    # device setting
    device = 'cpu'
    if torch.cuda.is_available():
        device = 'cuda'


    style_transfer = style_transfer.to(device)
    content_image = content_image.to(device)
    style_image = style_image.to(device)
    
    # noise
    # x = torch.randn(1,3,512,512).to(device) ë…¸ì´ì¦ˆ ì‹œì‘ì‚¬ì§„
    x = content_image.clone()                #ê³ ì–‘ì´ ì‹œì‘ì‚¬ì§„
    x.requires_grad_(True)

    # setting optimizer
    optimizer = optim.Adam([x], lr=lr)

    # train loop
    steps = 500
    for step in tqdm(range(steps)):
        ## content representation (x, content_image)
        ## style representation (x, style_image)

        x_content_list = style_transfer(x, 'content')
        y_content_list = style_transfer(content_image, 'content')

        x_style_list = style_transfer(x, 'style')
        y_style_list = style_transfer(style_image, 'style')

        ## loss_content, loss_style
        loss_c = 0
        loss_s = 0
        loss_total = 0

        for x_content, y_content in zip(x_content_list, y_content_list):
            loss_c += content_loss(x_content, y_content)
        loss_c = alpha*loss_c

        for x_style, y_style in zip(x_style_list, y_style_list):
            loss_s += style_loss(x_style, y_style)
        loss_s = beta*loss_s

        loss_total = loss_c + loss_s

        ## optimizer step
        optimizer.zero_grad()
        loss_total.backward()
        optimizer.step()

        ## loss print
        if step%100==0:
            print(f"loss_c: {loss_c.cpu()}")
            print(f"loss_s: {loss_s.cpu()}")
            print(f"loss_total: {loss_total.cpu()}")

            ## post processing
            ## image gen output save
            gen_img:Image.Image = post_processing(x)
            gen_img.save(os.path.join(save_root, f'{step}.jpg'))

if __name__=="__main__":
    train_main()
```

--------

## ìˆ˜í–‰ê²°ê³¼

- ìˆ˜í–‰ì€ codeëŠ” colabì—ì„œ ì‹¤í–‰ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤

